FramePack/demo_gradio.py is the main entry point for the image-to-video Gradio demo. The script boots a HunyuanVideo pipeline with safe Hugging Face login/download helpers, installs a proxy-friendly Gradio patch, and wires up runtime persistence for caches stored under `FramePack/Cache`. It defines utilities for hashing tensors, reconstructing nested outputs, and wrapping arbitrary modules in on-disk LRU caches that can operate in deterministic (“hash”) or FAISS-backed semantic modes. These wrappers are shared across the text encoders, SigLip image encoder, and transformer so prompts or conditioning frames can be reused across runs.

A large CLI surface (TensorRT toggles, cache controls, quantization switches, etc.) is validated up front to warn about conflicting combinations like `torch.compile` with the async memory backend. The models themselves (two text encoders, VAE, SigLip encoder, transformer) are downloaded or pulled from local snapshots, optionally loaded in parallel, and prepared with BitsAndBytes, FP8, quantization, or TensorRT acceleration depending on the flags. Additional optimizations—first-block cache, similarity cache, KV cache, adaptive latent windows, chunked VAE decode, memory-preserving offloads—are orchestrated so the pipeline can adapt between high- and low-VRAM GPUs.

Inference work happens inside the `worker` coroutine: prompts are encoded, HunyuanVideo latents are sampled via `sample_hunyuan`, and latents are decoded in chunks to MP4 segments while cache hit timelines stream back through an AsyncStream queue. The `process` generator consumes that queue to feed Gradio updates (video previews, progress bars, timeline markdown). Finally, a `gr.Blocks` UI exposes image upload, prompt boxes, sampler controls, cache/quality toggles, TensorRT switches, and status panels, and `block.launch` hosts the app with the requested server/port/sharing settings.
